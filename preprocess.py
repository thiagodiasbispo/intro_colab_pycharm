# -*- coding: utf-8 -*-
"""Iniciando ao Processamento de Linguagem Natural (PLN).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UaiXyYtZTULi4YHgaM-TlS4DF37o9tr8

# Processamento de linguagem Natural
"""
import re
from string import punctuation

import spacy
from spacy.lang.en import English
from spacy.lang.pt import Portuguese

"""# Baixando pipeline do spacy"""

"""# Lendo e explorando a base de dados"""

nlp = spacy.load("en_core_web_sm")

"""## Tokeninzando"""

spacy_tokenizer = nlp.tokenizer

def tokenize(text, lemmatize=False):
    if lemmatize: 
      return [w.lemma_ for w in nlp(text)
              if w.text.strip() and w.text not in stopwords]
    
    return [w.text.strip() for w in spacy_tokenizer(text) 
              if w.text.strip() and w.text not in stopwords]

"""## Removendo sinais de pontuação"""

def remove_punctuation(text):
    no_punct = [c for c in text if c not in punctuation]
    no_punct = "".join(no_punct)
    return no_punct

"""## Removendo stowords


"""

"; ".join(Portuguese.Defaults.stop_words)

stopwords = English.Defaults.stop_words

"; ".join(stopwords)

"""## Removendo tags html"""

def remove_html_tags(text):
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

"""## Definindo função de pré-processamento"""

def preprocess(text):
    no_html = remove_html_tags(text)
    no_punct =  remove_punctuation(no_html).lower()

    return no_punct

"""## Unindo tudo numa função única"""

def pipeline(texts, lemmatize=False):
    texts = (preprocess(t) for t in texts)
    token_texts = [tokenize(t, lemmatize) for t in texts]

    return token_texts
